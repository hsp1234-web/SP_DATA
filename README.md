# 數據整合平台 v15

## 核心功能

本數據整合管道 v13 的核心架構 (Odin Architecture) 帶來以下關鍵特性，確保了數據處理的彈性與韌性：

*   **容錯式綱要適應 (Fault-Tolerant Schema Adaptation)**：
    *   **告別僵化檢查**：不再因缺少次要欄位而輕易放棄整個檔案，大幅提升數據納入的成功率。
    *   **智慧結構補完**：在正規化欄位後，系統會自動將讀取的數據擴展至目標表格的完整綱要結構，任何缺失的欄位均以 `NULL` 值填充，確保結構一致性。
*   **本地優先工作流程 (Local-First Workflow)**：
    *   **提升效能與穩定性**：優先將 Google Drive 的數據同步至本地端進行處理，降低對網路連線的依賴，並提升處理速度。
    *   **資源隔離**：本地處理確保雲端資源的整潔，並在完成後才將結果同步回傳。
*   **可中斷續行的冪等設計 (Idempotent & Resumable Workflow)**：
    *   **進度追蹤**：透過檔案雜湊值 (hash) 精準追蹤已處理的檔案，避免重複作業。
    *   **斷點續行**：若處理流程意外中斷，重啟後能從上次的進度繼續執行，無需從頭來過（在 `NORMAL` 模式下）。
    *   **數據一致性**：資料庫載入操作具備冪等性，重複執行相同的載入指令不會產生副作用或重複數據（依據設定的衝突解決策略）。

## 架構亮點

此數據整合管道經過精心設計，旨在高效且穩健地處理大量且格式多樣的金融數據。其主要架構亮點包括：

*   **高效能數據處理**：
    *   採用 **DuckDB** 作為核心分析引擎，充分利用其針對分析型工作負載的優化，實現快速的數據查詢與轉換。
    *   中繼暫存階段使用 **Apache Parquet** 格式，提供高效的壓縮與欄式儲存，加快讀寫速度並節省儲存空間。
    *   透過 **多進程平行處理 (ProcessPoolExecutor)** 解析檔案，最大化利用 CPU 資源，縮短整體處理時間。
*   **穩健的錯誤處理與容錯機制**：
    *   **檔案層級錯誤隔離**：單一檔案的解析或處理失敗不會影響其他檔案的處理流程。失敗的檔案會被記錄並移至特定目錄，便於後續追蹤。
    *   **綱要自動適應與補完**：如「核心功能」所述，系統能智慧適應不同來源的欄位差異，並自動補齊至目標綱要，大幅降低因數據源微小變動導致的流程中斷。
*   **專為金融數據設計**：
    *   **多樣化金融數據格式支援**：內建邏輯可處理常見的金融數據檔案（如 CSV），並能透過綱要定義擴展至不同類型的報告（例如日報、週報）。
    *   **可配置的綱要管理**：允許使用者透過 JSON 設定檔定義不同數據源的綱要 (schema)、關鍵字及欄位對應關係，具備高度靈活性以應對金融市場數據的多變性。
*   **資源監控與日誌記錄**：
    *   內建硬體資源監控 (CPU, RAM, Disk)，即時掌握系統負載。
    *   詳細的執行日誌，記錄每一步操作的狀態、成功、警告或錯誤訊息，方便問題排查與流程審計。

## 如何執行 (Google Colab)

本數據整合管道主要設計於 Google Colab 環境中執行。以下為基本設定與執行步驟：

1.  **環境準備**：
    *   確保您的 Google Drive 中已建立專案主資料夾，並在其中包含一個名為 `01_input_files` 的子資料夾。所有待處理的原始數據檔案（CSV 或包含 CSV 的 ZIP 檔）都應放置於此 `01_input_files` 資料夾內。
    *   例如，若您的 `project_name` 設定為 `"MyTaifexDataProject"`，則輸入路徑應為 `My Drive/MyTaifexDataProject/01_input_files/`。

2.  **開啟 Colab 筆記本**：
    *   將此 Python 腳本上傳至 Google Colab 或作為新的 Colab 筆記本開啟。

3.  **進行執行設定**：
    *   在筆記本頂部的設定區塊，根據您的需求修改以下參數：
        *   `project_name`：您的專案名稱。這將決定在 Google Drive 中讀取數據和儲存結果的路徑。
        *   `conflict_strategy`：數據衝突時的處理策略。
            *   `"REPLACE"`：若數據已存在（根據唯一鍵判斷），則以新數據覆蓋。
            *   `"IGNORE"`：若數據已存在，則忽略新數據。
            *   `"FLAG_FOR_REVIEW"`：（在此 v13 版本中，此選項的具體行為可能未完全實作，通常 `REPLACE` 或 `IGNORE` 更常用於批次處理。）
        *   `run_mode`：執行模式。
            *   `"NORMAL"`：正常模式，僅處理自上次成功執行以來新增或變更的檔案。
            *   `"BACKFILL"`：歷史回填模式，將重新處理 `01_input_files` 中的所有檔案，無論之前是否已處理過。

4.  **執行所有儲存格**：
    *   點擊 Colab 介面中的「執行階段」 (Runtime) -> 「全部執行」 (Run all)。
    *   腳本將會：
        1.  提示掛載您的 Google Drive。請依指示授權存取。
        2.  檢查輸入資料夾是否存在，若不存在，會詢問是否自動建立。
        3.  設定本地工作區，並從 Google Drive 同步輸入檔案與現有資料庫（若有）。
        4.  解析新的數據檔案，轉換為 Parquet 格式並儲存於暫存區。
        5.  將處理後的數據載入 DuckDB 資料庫。
        6.  將更新後的資料庫檔案、處理記錄 (manifest) 和執行日誌回存至您 Google Drive 的專案資料夾內。
        7.  清理本地臨時工作區。

5.  **檢視結果**：
    *   執行完畢後，您可以在 Google Drive 的專案資料夾下找到：
        *   `03_database_output/taifex_analytics_v13.0.duckdb`：整合後的 DuckDB 資料庫檔案。
        *   `04_manifests/file_manifest.json`：已處理檔案的記錄。
        *   `執行日誌_v13.0_[時間戳記].txt`：詳細的執行日誌檔案。
        *   `05_failed_files/`：若有處理失敗的檔案，會存放於此。

**重要提示**：
*   首次執行或 `project_name` 變更後，由於需要建立資料夾結構與同步，執行時間可能較長。
*   確保您的 Google Drive 有足夠的儲存空間。

## 版本說明

*   **主版本**：v15
*   **版本號策略**：本專案遵循語意化版本控制 (Semantic Versioning) 原則。
    *   **補丁版本 (Patch Releases)**：例如 `v15.0.1`, `v15.0.2`，主要用於向後相容的錯誤修復。
    *   **次版本 (Minor Releases)**：例如 `v15.1.0`, `v15.2.0`，主要用於新增向後相容的功能。
    *   **主版本 (Major Releases)**：例如 `v16.0.0`，用於任何不向後相容的重大變更。

目前您檢視的這份說明文件對應的是 **數據整合平台 v15** 的功能與架構。
